You’re storing only days that have ≥1 case in Postgres (sparse series). In generate_fixed_predictions, you check len(nbhd_data) before densifying the time series. That makes it look like each neighborhood has “just a few days of data,” so lots of neighborhoods either get skipped or fall back to the simple forecast.

The fix (densify first)

Move _ensure_continuous_days(...) up so it runs before any length checks. That way, missing days are filled with zeros and the model “sees” the full span.

Replace this block in FixedSF311Pipeline.generate_fixed_predictions:

nbhd_data = historical_data[historical_data['neighborhood'] == neighborhood].copy()
nbhd_data = nbhd_data.sort_values('date').reset_index(drop=True)

# Sanity-log that each neighborhood truly sees 5y
min_date = nbhd_data['date'].min()
max_date = nbhd_data['date'].max()
...
MIN_TRAIN_DAYS = 30
if len(nbhd_data) < MIN_TRAIN_DAYS:
    continue

nbhd_data = self._ensure_continuous_days(nbhd_data)

if len(nbhd_data) < 60:
    forecast = self._simple_neighborhood_forecast(neighborhood, nbhd_data, prediction_days)
    all_forecasts.append(forecast)
    continue


with this:

nbhd_data = historical_data[historical_data['neighborhood'] == neighborhood].copy()
nbhd_data = nbhd_data.sort_values('date').reset_index(drop=True)

# DENSIFY FIRST – fill missing dates with 0 cases so the length reflects the true time span
nbhd_data = self._ensure_continuous_days(nbhd_data)

# Sanity-log with the densified series
min_date = nbhd_data['date'].min()
max_date = nbhd_data['date'].max()
if hasattr(min_date, 'date'): min_date = min_date.date()
if hasattr(max_date, 'date'): max_date = max_date.date()
print(f"[{neighborhood}] train span (densified): {min_date} → {max_date} | rows: {len(nbhd_data)}")

# Now length checks make sense
MIN_TRAIN_DAYS = 30
if len(nbhd_data) < MIN_TRAIN_DAYS:
    continue

if len(nbhd_data) < 60:
    forecast = self._simple_neighborhood_forecast(neighborhood, nbhd_data, prediction_days)
    all_forecasts.append(forecast)
    continue

Why this works

Your DB only has rows for “active” days.

Before: len(nbhd_data) might be tiny (e.g., 200 rows across 5 years) → models think there’s little history.

After: _ensure_continuous_days builds a full daily index (≈ 1825 rows per neighborhood), with zeros where no cases occurred. Now backtesting/ML gets the real timeline.

Optional hardening (nice-to-haves)

In _ensure_continuous_days, cap the range to the last 5 years (prevents extreme spans if your cache grows):

def _ensure_continuous_days(self, df: pd.DataFrame) -> pd.DataFrame:
    df['date'] = pd.to_datetime(df['date'])
    start = max(df['date'].min(), pd.Timestamp.today().normalize() - pd.Timedelta(days=1825))
    end = df['date'].max()
    date_range = pd.date_range(start=start, end=end, freq='D')
    ...


In generate_predictions_with_cache, normalize types once so downstream code is consistent:

historical_data = self.fetch_and_cache_data(target_days, force_refresh)
historical_data = historical_data.copy()
historical_data['date'] = pd.to_datetime(historical_data['date']).dt.normalize()
historical_data['neighborhood'] = historical_data['neighborhood'].astype(str)
historical_data['cases'] = historical_data['cases'].fillna(0).astype(int)